/home/chwolters/BachelorThesis/Training_pytorch/log/default/ADCprecision=5/batch_size=200/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5
Building CIFAR-10 data loader with 1 workers
Files already downloaded and verified
Files already downloaded and verified
fan_in     27, float_limit 0.333333, quant limit 1.5, scale 4
fan_in   1152, float_limit 0.051031, quant limit 1.5, scale 32
fan_in   1152, float_limit 0.051031, quant limit 1.5, scale 32
fan_in   2304, float_limit 0.036084, quant limit 1.5, scale 32
fan_in   2304, float_limit 0.036084, quant limit 1.5, scale 32
fan_in   4608, float_limit 0.025516, quant limit 1.5, scale 64
fan_in   8192, float_limit 0.019137, quant limit 1.5, scale 64
fan_in   1024, float_limit 0.054127, quant limit 1.5, scale 32
weight distribution
[-4.71894396e-03 -3.68156005e-03  5.64123271e-03 -3.65321338e-03
 -8.04464726e-05 -1.32363476e-02 -3.35567445e-02 -3.59818875e-03
  4.93629158e-01  6.24858081e-01  6.38363004e-01  6.56319678e-01
  6.65003657e-01  6.55160606e-01  7.00596809e-01  6.37180090e-01]
delta distribution
[-5.29875560e-03  2.08960637e-03  1.17874146e-03  8.66042275e-04
  4.10874694e-04  4.92890656e-04  8.07046890e-05 -4.88281257e-05
  3.00024003e-02  1.98736228e-02  1.84721202e-02  1.24577628e-02
  1.08058155e-02  1.09312814e-02  6.16319198e-03  1.09099858e-02]
------------------------------ FloorPlan --------------------------------

Tile and PE size are optimized to maximize memory utilization ( = memory mapped by synapse / total memory on chip)

Desired Conventional Mapped Tile Storage Size: 1024x1024
Desired Conventional PE Storage Size: 512x512
Desired Novel Mapped Tile Storage Size: 9x512x512
User-defined SubArray Size: 128x128

----------------- # of tile used for each layer -----------------
layer1: 1
layer2: 1
layer3: 1
layer4: 1
layer5: 1
layer6: 1
layer7: 8
layer8: 1

----------------- Speed-up of each layer ------------------
layer1: 64
layer2: 16
layer3: 8
layer4: 4
layer5: 2
layer6: 1
layer7: 1
layer8: 8

----------------- Utilization of each layer ------------------
layer1: 0.210938
layer2: 1
layer3: 1
layer4: 1
layer5: 1
layer6: 1
layer7: 1
layer8: 0.078125
Memory Utilization of Whole Chip: 88.5938 % 

---------------------------- FloorPlan Done ------------------------------



-------------------------------------- Hardware Performance --------------------------------------
-------------------- Estimation of Layer 1 ----------------------
Hier gehts los mit den Types
<class '_io.BufferedWriter'>
<class 'int'>
<class 'torch.Tensor'>
<class 'numpy.ndarray'>
