=================FLAGS==================
type: cifar10
batch_size: 200
epochs: 50
grad_scale: 1
seed: 117
log_interval: 100
test_interval: 50
logdir: /home/chwolters/BachelorThesis/Training_pytorch/log/default/ADCprecision=5/batch_size=200/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5
decreasing_lr: 200,250
wl_weight: 5
wl_grad: 5
wl_activate: 8
wl_error: 8
inference: 0
onoffratio: 10
cellBit: 5
subArray: 128
ADCprecision: 5
vari: 0
t: 0
v: 0
detect: 0
target: 0
nonlinearityLTP: 1.75
nonlinearityLTD: 1.46
max_level: 32
d2dVari: 0.0
c2cVari: 0.003
========================================
Sequential(
  (0): QConv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): ReLU()
  (2): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (3): ReLU()
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): QConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (6): ReLU()
  (7): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (8): ReLU()
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): QConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (11): ReLU()
  (12): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (13): ReLU()
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Sequential(
  (0): QLinear(in_features=8192, out_features=1024, bias=False)
  (1): ReLU(inplace=True)
  (2): QLinear(in_features=1024, out_features=10, bias=False)
)
decreasing_lr: [200, 250]
training phase
Train Epoch: 0 [20000/50000] Loss: 83.652206 Acc: 0.2750 lr: 1.00e+00
Train Epoch: 0 [40000/50000] Loss: 74.626762 Acc: 0.4200 lr: 1.00e+00
Elapsed 120.85s, 120.85 s/epoch, 0.48 s/batch, ets 5921.50s
training phase
Train Epoch: 1 [20000/50000] Loss: 69.825851 Acc: 0.4850 lr: 1.00e+00
Train Epoch: 1 [40000/50000] Loss: 69.831604 Acc: 0.4600 lr: 1.00e+00
Elapsed 234.97s, 117.48 s/epoch, 0.47 s/batch, ets 5639.24s
training phase
Train Epoch: 2 [20000/50000] Loss: 64.669083 Acc: 0.5100 lr: 1.00e+00
Train Epoch: 2 [40000/50000] Loss: 56.873940 Acc: 0.5550 lr: 1.00e+00
Elapsed 349.38s, 116.46 s/epoch, 0.47 s/batch, ets 5473.63s
training phase
Train Epoch: 3 [20000/50000] Loss: 67.771210 Acc: 0.4800 lr: 1.00e+00
Train Epoch: 3 [40000/50000] Loss: 51.315376 Acc: 0.6400 lr: 1.00e+00
Elapsed 463.97s, 115.99 s/epoch, 0.46 s/batch, ets 5335.64s
training phase
Train Epoch: 4 [20000/50000] Loss: 51.426510 Acc: 0.6350 lr: 1.00e+00
Train Epoch: 4 [40000/50000] Loss: 55.795418 Acc: 0.6000 lr: 1.00e+00
Elapsed 579.60s, 115.92 s/epoch, 0.46 s/batch, ets 5216.42s
training phase
Train Epoch: 5 [20000/50000] Loss: 51.794601 Acc: 0.5950 lr: 1.00e+00
Train Epoch: 5 [40000/50000] Loss: 42.713524 Acc: 0.7150 lr: 1.00e+00
Elapsed 695.09s, 115.85 s/epoch, 0.46 s/batch, ets 5097.31s
training phase
Train Epoch: 6 [20000/50000] Loss: 47.277718 Acc: 0.6550 lr: 1.00e+00
Train Epoch: 6 [40000/50000] Loss: 51.088749 Acc: 0.6350 lr: 1.00e+00
Elapsed 809.95s, 115.71 s/epoch, 0.46 s/batch, ets 4975.39s
training phase
Train Epoch: 7 [20000/50000] Loss: 44.860931 Acc: 0.6700 lr: 1.00e+00
Train Epoch: 7 [40000/50000] Loss: 44.010727 Acc: 0.6900 lr: 1.00e+00
Elapsed 924.52s, 115.56 s/epoch, 0.46 s/batch, ets 4853.72s
training phase
Train Epoch: 8 [20000/50000] Loss: 37.045738 Acc: 0.7650 lr: 1.00e+00
Train Epoch: 8 [40000/50000] Loss: 39.059807 Acc: 0.7400 lr: 1.00e+00
Elapsed 1039.92s, 115.55 s/epoch, 0.46 s/batch, ets 4737.41s
training phase
Train Epoch: 9 [20000/50000] Loss: 43.531487 Acc: 0.6900 lr: 1.00e+00
Train Epoch: 9 [40000/50000] Loss: 39.007023 Acc: 0.7450 lr: 1.00e+00
Elapsed 1154.62s, 115.46 s/epoch, 0.46 s/batch, ets 4618.47s
training phase
Train Epoch: 10 [20000/50000] Loss: 33.656883 Acc: 0.7950 lr: 1.00e+00
Train Epoch: 10 [40000/50000] Loss: 29.867977 Acc: 0.8150 lr: 1.00e+00
Elapsed 1268.90s, 115.35 s/epoch, 0.46 s/batch, ets 4498.83s
training phase
Train Epoch: 11 [20000/50000] Loss: 28.249828 Acc: 0.8450 lr: 1.00e+00
Train Epoch: 11 [40000/50000] Loss: 33.167988 Acc: 0.7700 lr: 1.00e+00
Elapsed 1382.21s, 115.18 s/epoch, 0.46 s/batch, ets 4377.00s
training phase
Train Epoch: 12 [20000/50000] Loss: 39.173225 Acc: 0.7450 lr: 1.00e+00
Train Epoch: 12 [40000/50000] Loss: 31.030209 Acc: 0.8000 lr: 1.00e+00
Elapsed 1489.62s, 114.59 s/epoch, 0.46 s/batch, ets 4239.70s
training phase
Train Epoch: 13 [20000/50000] Loss: 36.424263 Acc: 0.7500 lr: 1.00e+00
Train Epoch: 13 [40000/50000] Loss: 35.319557 Acc: 0.7750 lr: 1.00e+00
Elapsed 1594.63s, 113.90 s/epoch, 0.46 s/batch, ets 4100.47s
training phase
Train Epoch: 14 [20000/50000] Loss: 30.890949 Acc: 0.8100 lr: 1.00e+00
Train Epoch: 14 [40000/50000] Loss: 33.523670 Acc: 0.7750 lr: 1.00e+00
Elapsed 1703.18s, 113.55 s/epoch, 0.45 s/batch, ets 3974.08s
training phase
Train Epoch: 15 [20000/50000] Loss: 25.910851 Acc: 0.8250 lr: 1.00e+00
Train Epoch: 15 [40000/50000] Loss: 34.273453 Acc: 0.7500 lr: 1.00e+00
Elapsed 1815.08s, 113.44 s/epoch, 0.45 s/batch, ets 3857.05s
training phase
Train Epoch: 16 [20000/50000] Loss: 27.929424 Acc: 0.8400 lr: 1.00e+00
Train Epoch: 16 [40000/50000] Loss: 38.294006 Acc: 0.7450 lr: 1.00e+00
Elapsed 1925.65s, 113.27 s/epoch, 0.45 s/batch, ets 3738.02s
training phase
Train Epoch: 17 [20000/50000] Loss: 23.297424 Acc: 0.8600 lr: 1.00e+00
Train Epoch: 17 [40000/50000] Loss: 33.074432 Acc: 0.7900 lr: 1.00e+00
Elapsed 2035.57s, 113.09 s/epoch, 0.45 s/batch, ets 3618.79s
training phase
Train Epoch: 18 [20000/50000] Loss: 26.504528 Acc: 0.8450 lr: 1.00e+00
Train Epoch: 18 [40000/50000] Loss: 28.567001 Acc: 0.8150 lr: 1.00e+00
Elapsed 2147.78s, 113.04 s/epoch, 0.45 s/batch, ets 3504.28s
training phase
Train Epoch: 19 [20000/50000] Loss: 28.383488 Acc: 0.8350 lr: 1.00e+00
Train Epoch: 19 [40000/50000] Loss: 31.767477 Acc: 0.7950 lr: 1.00e+00
Elapsed 2262.22s, 113.11 s/epoch, 0.45 s/batch, ets 3393.33s
training phase
Train Epoch: 20 [20000/50000] Loss: 33.383888 Acc: 0.7850 lr: 1.00e+00
Train Epoch: 20 [40000/50000] Loss: 29.504009 Acc: 0.8150 lr: 1.00e+00
Elapsed 2377.35s, 113.21 s/epoch, 0.45 s/batch, ets 3283.01s
training phase
Train Epoch: 21 [20000/50000] Loss: 31.957108 Acc: 0.7950 lr: 1.00e+00
Train Epoch: 21 [40000/50000] Loss: 30.744900 Acc: 0.8100 lr: 1.00e+00
Elapsed 2487.89s, 113.09 s/epoch, 0.45 s/batch, ets 3166.40s
training phase
Train Epoch: 22 [20000/50000] Loss: 24.159863 Acc: 0.8650 lr: 1.00e+00
Train Epoch: 22 [40000/50000] Loss: 31.453487 Acc: 0.7800 lr: 1.00e+00
Elapsed 2601.78s, 113.12 s/epoch, 0.45 s/batch, ets 3054.26s
training phase
Train Epoch: 23 [20000/50000] Loss: 33.776180 Acc: 0.7750 lr: 1.00e+00
Train Epoch: 23 [40000/50000] Loss: 25.402626 Acc: 0.8450 lr: 1.00e+00
Elapsed 2717.60s, 113.23 s/epoch, 0.45 s/batch, ets 2944.06s
training phase
Train Epoch: 24 [20000/50000] Loss: 29.386105 Acc: 0.8300 lr: 1.00e+00
Train Epoch: 24 [40000/50000] Loss: 25.253679 Acc: 0.8350 lr: 1.00e+00
Elapsed 2832.20s, 113.29 s/epoch, 0.45 s/batch, ets 2832.20s
training phase
Train Epoch: 25 [20000/50000] Loss: 28.339323 Acc: 0.8300 lr: 1.00e+00
Train Epoch: 25 [40000/50000] Loss: 28.625769 Acc: 0.8200 lr: 1.00e+00
Elapsed 2944.47s, 113.25 s/epoch, 0.45 s/batch, ets 2717.97s
training phase
Train Epoch: 26 [20000/50000] Loss: 27.244511 Acc: 0.8300 lr: 1.00e+00
Train Epoch: 26 [40000/50000] Loss: 31.698147 Acc: 0.7800 lr: 1.00e+00
Elapsed 3058.47s, 113.28 s/epoch, 0.45 s/batch, ets 2605.36s
training phase
Train Epoch: 27 [20000/50000] Loss: 27.517754 Acc: 0.8450 lr: 1.00e+00
Train Epoch: 27 [40000/50000] Loss: 28.197573 Acc: 0.8350 lr: 1.00e+00
Elapsed 3171.95s, 113.28 s/epoch, 0.45 s/batch, ets 2492.25s
training phase
Train Epoch: 28 [20000/50000] Loss: 27.395664 Acc: 0.8150 lr: 1.00e+00
Train Epoch: 28 [40000/50000] Loss: 25.282255 Acc: 0.8400 lr: 1.00e+00
Elapsed 3286.72s, 113.34 s/epoch, 0.45 s/batch, ets 2380.04s
training phase
Train Epoch: 29 [20000/50000] Loss: 20.220356 Acc: 0.9000 lr: 1.00e+00
Train Epoch: 29 [40000/50000] Loss: 30.982292 Acc: 0.7900 lr: 1.00e+00
Elapsed 3401.97s, 113.40 s/epoch, 0.45 s/batch, ets 2267.98s
training phase
Train Epoch: 30 [20000/50000] Loss: 24.730648 Acc: 0.8400 lr: 1.00e+00
Train Epoch: 30 [40000/50000] Loss: 24.685997 Acc: 0.8300 lr: 1.00e+00
Elapsed 3516.66s, 113.44 s/epoch, 0.45 s/batch, ets 2155.37s
training phase
Train Epoch: 31 [20000/50000] Loss: 23.877371 Acc: 0.8500 lr: 1.00e+00
Train Epoch: 31 [40000/50000] Loss: 26.198715 Acc: 0.8450 lr: 1.00e+00
Elapsed 3629.97s, 113.44 s/epoch, 0.45 s/batch, ets 2041.86s
training phase
Train Epoch: 32 [20000/50000] Loss: 23.887150 Acc: 0.8550 lr: 1.00e+00
Train Epoch: 32 [40000/50000] Loss: 30.231470 Acc: 0.7950 lr: 1.00e+00
Elapsed 3745.77s, 113.51 s/epoch, 0.45 s/batch, ets 1929.64s
training phase
Train Epoch: 33 [20000/50000] Loss: 28.811886 Acc: 0.7850 lr: 1.00e+00
Train Epoch: 33 [40000/50000] Loss: 29.489481 Acc: 0.8150 lr: 1.00e+00
Elapsed 3860.03s, 113.53 s/epoch, 0.45 s/batch, ets 1816.48s
training phase
Train Epoch: 34 [20000/50000] Loss: 28.484051 Acc: 0.8000 lr: 1.00e+00
Train Epoch: 34 [40000/50000] Loss: 27.475355 Acc: 0.8000 lr: 1.00e+00
Elapsed 3974.84s, 113.57 s/epoch, 0.45 s/batch, ets 1703.50s
training phase
Train Epoch: 35 [20000/50000] Loss: 27.210152 Acc: 0.8200 lr: 1.00e+00
Train Epoch: 35 [40000/50000] Loss: 21.536238 Acc: 0.8600 lr: 1.00e+00
Elapsed 4089.59s, 113.60 s/epoch, 0.45 s/batch, ets 1590.40s
training phase
Train Epoch: 36 [20000/50000] Loss: 26.830978 Acc: 0.8500 lr: 1.00e+00
Train Epoch: 36 [40000/50000] Loss: 22.609909 Acc: 0.8800 lr: 1.00e+00
Elapsed 4204.62s, 113.64 s/epoch, 0.45 s/batch, ets 1477.30s
training phase
Train Epoch: 37 [20000/50000] Loss: 23.940117 Acc: 0.8650 lr: 1.00e+00
Train Epoch: 37 [40000/50000] Loss: 28.321079 Acc: 0.8250 lr: 1.00e+00
Elapsed 4318.75s, 113.65 s/epoch, 0.45 s/batch, ets 1363.81s
training phase
Train Epoch: 38 [20000/50000] Loss: 29.016891 Acc: 0.8000 lr: 1.00e+00
Train Epoch: 38 [40000/50000] Loss: 29.121161 Acc: 0.7800 lr: 1.00e+00
Elapsed 4431.87s, 113.64 s/epoch, 0.45 s/batch, ets 1250.02s
training phase
Train Epoch: 39 [20000/50000] Loss: 24.543722 Acc: 0.8400 lr: 1.00e+00
Train Epoch: 39 [40000/50000] Loss: 23.293245 Acc: 0.8450 lr: 1.00e+00
Elapsed 4547.33s, 113.68 s/epoch, 0.45 s/batch, ets 1136.83s
training phase
Train Epoch: 40 [20000/50000] Loss: 25.274162 Acc: 0.8550 lr: 1.00e+00
Train Epoch: 40 [40000/50000] Loss: 23.432667 Acc: 0.8600 lr: 1.00e+00
Elapsed 4662.30s, 113.71 s/epoch, 0.45 s/batch, ets 1023.43s
training phase
Train Epoch: 41 [20000/50000] Loss: 29.619654 Acc: 0.8000 lr: 1.00e+00
Train Epoch: 41 [40000/50000] Loss: 32.517117 Acc: 0.7850 lr: 1.00e+00
Elapsed 4777.54s, 113.75 s/epoch, 0.46 s/batch, ets 910.01s
training phase
Train Epoch: 42 [20000/50000] Loss: 30.503788 Acc: 0.7900 lr: 1.00e+00
Train Epoch: 42 [40000/50000] Loss: 29.821569 Acc: 0.8300 lr: 1.00e+00
Elapsed 4892.33s, 113.78 s/epoch, 0.46 s/batch, ets 796.43s
training phase
Train Epoch: 43 [20000/50000] Loss: 25.506659 Acc: 0.8300 lr: 1.00e+00
Train Epoch: 43 [40000/50000] Loss: 25.594318 Acc: 0.8300 lr: 1.00e+00
Elapsed 5006.48s, 113.78 s/epoch, 0.46 s/batch, ets 682.70s
training phase
Train Epoch: 44 [20000/50000] Loss: 30.064161 Acc: 0.8250 lr: 1.00e+00
Train Epoch: 44 [40000/50000] Loss: 23.447063 Acc: 0.8650 lr: 1.00e+00
Elapsed 5121.63s, 113.81 s/epoch, 0.46 s/batch, ets 569.07s
training phase
Train Epoch: 45 [20000/50000] Loss: 26.014496 Acc: 0.8550 lr: 1.00e+00
Train Epoch: 45 [40000/50000] Loss: 19.928493 Acc: 0.8950 lr: 1.00e+00
Elapsed 5236.21s, 113.83 s/epoch, 0.46 s/batch, ets 455.32s
training phase
Train Epoch: 46 [20000/50000] Loss: 30.056650 Acc: 0.8000 lr: 1.00e+00
Train Epoch: 46 [40000/50000] Loss: 24.758455 Acc: 0.8400 lr: 1.00e+00
Elapsed 5349.26s, 113.81 s/epoch, 0.46 s/batch, ets 341.44s
training phase
Train Epoch: 47 [20000/50000] Loss: 20.283840 Acc: 0.8850 lr: 1.00e+00
Train Epoch: 47 [40000/50000] Loss: 22.546146 Acc: 0.8800 lr: 1.00e+00
Elapsed 5464.66s, 113.85 s/epoch, 0.46 s/batch, ets 227.69s
training phase
Train Epoch: 48 [20000/50000] Loss: 26.658913 Acc: 0.8250 lr: 1.00e+00
Train Epoch: 48 [40000/50000] Loss: 25.103485 Acc: 0.8650 lr: 1.00e+00
Elapsed 5579.56s, 113.87 s/epoch, 0.46 s/batch, ets 113.87s
training phase
Train Epoch: 49 [20000/50000] Loss: 34.011852 Acc: 0.7800 lr: 1.00e+00
Train Epoch: 49 [40000/50000] Loss: 26.510044 Acc: 0.8050 lr: 1.00e+00
Elapsed 5694.70s, 113.89 s/epoch, 0.46 s/batch, ets 0.00s
testing phase
	Epoch 49 Test set: Average loss: 26.9423, Accuracy: 8229/10000 (82%)
Saving model to /home/chwolters/BachelorThesis/Training_pytorch/log/default/ADCprecision=5/batch_size=200/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5/best-49.pth
