/home/chwolters/BachelorThesis/Training_pytorch/log/default/ADCprecision=5/batch_size=200/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5
Building CIFAR-10 data loader with 1 workers
Files already downloaded and verified
=================FLAGS==================
type: cifar10
batch_size: 200
epochs: 50
grad_scale: 1
seed: 117
log_interval: 100
test_interval: 50
logdir: /home/chwolters/BachelorThesis/Training_pytorch/log/default/ADCprecision=5/batch_size=200/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5
decreasing_lr: 200,250
wl_weight: 5
wl_grad: 5
wl_activate: 8
wl_error: 8
inference: 0
onoffratio: 10
cellBit: 5
subArray: 128
ADCprecision: 5
vari: 0
t: 0
v: 0
detect: 0
target: 0
nonlinearityLTP: 1.75
nonlinearityLTD: 1.46
max_level: 32
d2dVari: 0.0
c2cVari: 0.003
========================================
Sequential(
  (0): QConv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): ReLU()
  (2): QConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (3): ReLU()
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): QConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (6): ReLU()
  (7): QConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (8): ReLU()
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): QConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (11): ReLU()
  (12): QConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (13): ReLU()
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Sequential(
  (0): QLinear(in_features=8192, out_features=1024, bias=False)
  (1): ReLU(inplace=True)
  (2): QLinear(in_features=1024, out_features=10, bias=False)
)
Files already downloaded and verified
fan_in     27, float_limit 0.333333, quant limit 1.5, scale 4
fan_in   1152, float_limit 0.051031, quant limit 1.5, scale 32
fan_in   1152, float_limit 0.051031, quant limit 1.5, scale 32
fan_in   2304, float_limit 0.036084, quant limit 1.5, scale 32
fan_in   2304, float_limit 0.036084, quant limit 1.5, scale 32
fan_in   4608, float_limit 0.025516, quant limit 1.5, scale 64
fan_in   8192, float_limit 0.019137, quant limit 1.5, scale 64
fan_in   1024, float_limit 0.054127, quant limit 1.5, scale 32
decreasing_lr: [200, 250]
training phase
Train Epoch: 0 [20000/50000] Loss: 83.652206 Acc: 0.2750 lr: 1.00e+00
Train Epoch: 0 [40000/50000] Loss: 74.626762 Acc: 0.4200 lr: 1.00e+00
Elapsed 120.85s, 120.85 s/epoch, 0.48 s/batch, ets 5921.50s
weight distribution
[-4.71894396e-03 -3.68156005e-03  5.64123271e-03 -3.65321338e-03
 -8.04464726e-05 -1.32363476e-02 -3.35567445e-02 -3.59818875e-03
  4.93629158e-01  6.24858081e-01  6.38363004e-01  6.56319678e-01
  6.65003657e-01  6.55160606e-01  7.00596809e-01  6.37180090e-01]
delta distribution
[-5.29875560e-03  2.08960637e-03  1.17874146e-03  8.66042275e-04
  4.10874694e-04  4.92890656e-04  8.07046890e-05 -4.88281257e-05
  3.00024003e-02  1.98736228e-02  1.84721202e-02  1.24577628e-02
  1.08058155e-02  1.09312814e-02  6.16319198e-03  1.09099858e-02]
training phase
Train Epoch: 1 [20000/50000] Loss: 69.825851 Acc: 0.4850 lr: 1.00e+00
Train Epoch: 1 [40000/50000] Loss: 69.831604 Acc: 0.4600 lr: 1.00e+00
Elapsed 234.97s, 117.48 s/epoch, 0.47 s/batch, ets 5639.24s
weight distribution
[-0.00804417 -0.01001254  0.00940686 -0.00970166 -0.00088454 -0.02341184
 -0.03853624 -0.00347794  0.45017144  0.58527601  0.58480453  0.61462915
  0.62471086  0.62553185  0.69283247  0.63514471]
delta distribution
[-6.79976866e-03  3.45399650e-03  2.33650208e-03  2.74891336e-03
  3.99165699e-04  3.19798797e-04  1.83880329e-05 -1.22070312e-04
  2.88288295e-02  2.15574782e-02  2.05558576e-02  1.95195656e-02
  1.34190908e-02  1.46920178e-02  6.17591990e-03  8.60217772e-03]
training phase
Train Epoch: 2 [20000/50000] Loss: 64.669083 Acc: 0.5100 lr: 1.00e+00
Train Epoch: 2 [40000/50000] Loss: 56.873940 Acc: 0.5550 lr: 1.00e+00
Elapsed 349.38s, 116.46 s/epoch, 0.47 s/batch, ets 5473.63s
weight distribution
[-0.01926078 -0.02029577  0.00383346 -0.01976375 -0.00083698 -0.02703691
 -0.04123838 -0.00253593  0.4337694   0.55728167  0.55642468  0.59089637
  0.59450495  0.60468787  0.68724138  0.63507253]
delta distribution
[-4.14134841e-03  9.73171671e-04  5.59488952e-04  5.16573607e-04
  4.36570917e-05  1.03924009e-04  3.93912196e-05 -7.32421904e-05
  2.97691096e-02  1.84920039e-02  1.70084406e-02  1.68232359e-02
  1.67541970e-02  1.38078332e-02  6.38728170e-03  8.99300352e-03]
training phase
Train Epoch: 3 [20000/50000] Loss: 67.771210 Acc: 0.4800 lr: 1.00e+00
Train Epoch: 3 [40000/50000] Loss: 51.315376 Acc: 0.6400 lr: 1.00e+00
Elapsed 463.97s, 115.99 s/epoch, 0.46 s/batch, ets 5335.64s
weight distribution
[-0.01702468 -0.02419991 -0.003555   -0.03208438 -0.00359235 -0.03230149
 -0.0424977  -0.00099483  0.44894889  0.54402727  0.54214323  0.58488542
  0.57440209  0.58893859  0.68376714  0.63191003]
delta distribution
[-1.98929396e-04  3.65447998e-03  9.73807473e-04  1.12067326e-03
  6.59041922e-04  4.06371226e-04  9.74461436e-05 -4.27246086e-05
  3.62297669e-02  2.58405041e-02  2.02711914e-02  2.04509720e-02
  1.52350515e-02  1.24087008e-02  6.52115094e-03  8.66920128e-03]
training phase
Train Epoch: 4 [20000/50000] Loss: 51.426510 Acc: 0.6350 lr: 1.00e+00
Train Epoch: 4 [40000/50000] Loss: 55.795418 Acc: 0.6000 lr: 1.00e+00
Elapsed 579.60s, 115.92 s/epoch, 0.46 s/batch, ets 5216.42s
weight distribution
[-2.16385275e-02 -2.54653729e-02 -4.02976945e-03 -4.35563922e-02
 -4.57436359e-03 -3.48147862e-02 -4.61045876e-02 -3.93416733e-04
  4.62463707e-01  5.36799669e-01  5.32672822e-01  5.78250706e-01
  5.59717357e-01  5.76204717e-01  6.81989968e-01  6.28714740e-01]
delta distribution
[-2.69458909e-03  2.22651171e-03  6.61214173e-04  1.14048854e-03
  2.61094829e-04  1.64243920e-05  6.19813800e-05 -3.05175781e-04
  3.09377704e-02  1.88787747e-02  1.92804430e-02  2.06511337e-02
  1.96321644e-02  1.09570427e-02  6.48722053e-03  1.23494798e-02]
training phase
Train Epoch: 5 [20000/50000] Loss: 51.794601 Acc: 0.5950 lr: 1.00e+00
Train Epoch: 5 [40000/50000] Loss: 42.713524 Acc: 0.7150 lr: 1.00e+00
Elapsed 695.09s, 115.85 s/epoch, 0.46 s/batch, ets 5097.31s
weight distribution
[-0.01437223 -0.02808244 -0.00771688 -0.05377649 -0.00748406 -0.03850865
 -0.04905642 -0.00080891  0.47181046  0.5319925   0.52792144  0.57838446
  0.55043477  0.56619555  0.68223542  0.62612033]
delta distribution
[-2.22439249e-03 -7.04871316e-04  8.71022567e-05  1.58850348e-03
  1.92377309e-04 -4.38637217e-04 -2.95042992e-06 -2.07519537e-04
  2.98221633e-02  1.64059866e-02  2.35329755e-02  1.91660468e-02
  1.74501166e-02  1.54336151e-02  7.10056955e-03  6.76297070e-03]
training phase
Train Epoch: 6 [20000/50000] Loss: 47.277718 Acc: 0.6550 lr: 1.00e+00
Train Epoch: 6 [40000/50000] Loss: 51.088749 Acc: 0.6350 lr: 1.00e+00
Elapsed 809.95s, 115.71 s/epoch, 0.46 s/batch, ets 4975.39s
weight distribution
[-0.01241006 -0.0295225  -0.00621498 -0.05077038 -0.00922712 -0.04207201
 -0.05187762  0.00079374  0.46882835  0.52886832  0.5245716   0.5721243
  0.54313165  0.55843896  0.68064058  0.6236282 ]
delta distribution
[ 9.04224580e-05 -7.98119465e-04 -4.74717890e-05 -3.87191772e-04
 -8.84268011e-05 -1.58018534e-04  3.68282199e-05  1.22070312e-04
  2.92937383e-02  1.76420845e-02  1.85189303e-02  2.17493046e-02
  1.33449500e-02  1.64676830e-02  7.04837544e-03  1.07686222e-02]
training phase
Train Epoch: 7 [20000/50000] Loss: 44.860931 Acc: 0.6700 lr: 1.00e+00
Train Epoch: 7 [40000/50000] Loss: 44.010727 Acc: 0.6900 lr: 1.00e+00
Elapsed 924.52s, 115.56 s/epoch, 0.46 s/batch, ets 4853.72s
weight distribution
[-0.01330039 -0.03078982 -0.00879337 -0.05209945 -0.01182328 -0.04510297
 -0.05500082  0.00076222  0.46974581  0.52831423  0.52379692  0.57099605
  0.5394761   0.55315536  0.68150824  0.62174475]
delta distribution
[-1.46484375e-03  1.74204513e-04 -4.57975606e-04  1.59263611e-04
 -4.83989716e-04  1.48587758e-04  2.88560987e-05  7.32421904e-05
  2.52318662e-02  1.53977852e-02  2.28205249e-02  1.65878423e-02
  2.02712175e-02  1.11226011e-02  7.80072622e-03  1.11519461e-02]
training phase
Train Epoch: 8 [20000/50000] Loss: 37.045738 Acc: 0.7650 lr: 1.00e+00
Train Epoch: 8 [40000/50000] Loss: 39.059807 Acc: 0.7400 lr: 1.00e+00
Elapsed 1039.92s, 115.55 s/epoch, 0.46 s/batch, ets 4737.41s
weight distribution
[-0.01846915 -0.03183521 -0.01279051 -0.05965051 -0.01420812 -0.04887277
 -0.05669847  0.00166687  0.46949688  0.52733696  0.52378345  0.57282001
  0.53694177  0.5498848   0.68337828  0.61885101]
delta distribution
[-2.44140625e-03  8.20583780e-04  1.27156571e-04  7.49164145e-04
  8.57247287e-05  1.37752955e-04  4.54559922e-05 -3.05175781e-05
  3.18231881e-02  1.64642856e-02  2.29604449e-02  1.72996279e-02
  1.51624940e-02  1.38581078e-02  6.60416810e-03  9.78556741e-03]
training phase
Train Epoch: 9 [20000/50000] Loss: 43.531487 Acc: 0.6900 lr: 1.00e+00
Train Epoch: 9 [40000/50000] Loss: 39.007023 Acc: 0.7450 lr: 1.00e+00
Elapsed 1154.62s, 115.46 s/epoch, 0.46 s/batch, ets 4618.47s
weight distribution
[-0.00864206 -0.03637867 -0.0121932  -0.06594805 -0.01567883 -0.05397189
 -0.06025548  0.00327203  0.48335069  0.53037149  0.52118134  0.57135522
  0.5355227   0.54920894  0.68544894  0.61651909]
delta distribution
[-6.63700793e-03 -1.48349336e-05 -7.41746680e-06  6.41081075e-04
  4.76307359e-05  1.10281842e-04  1.68606639e-05  6.10351572e-06
  3.23569737e-02  1.74488500e-02  1.93565506e-02  2.17314195e-02
  1.16077894e-02  8.99864919e-03  7.47501291e-03  7.98195694e-03]
training phase
Train Epoch: 10 [20000/50000] Loss: 33.656883 Acc: 0.7950 lr: 1.00e+00
Train Epoch: 10 [40000/50000] Loss: 29.867977 Acc: 0.8150 lr: 1.00e+00
Elapsed 1268.90s, 115.35 s/epoch, 0.46 s/batch, ets 4498.83s
weight distribution
[-0.0080708  -0.03546014 -0.01423491 -0.06417344 -0.01672577 -0.05500676
 -0.062359    0.00483617  0.48031154  0.52756631  0.52203953  0.57318586
  0.53501642  0.54670173  0.68457723  0.61243641]
delta distribution
[-4.95515065e-03  5.63727481e-05  2.45624135e-04  1.80032526e-04
 -1.23818711e-04  2.30762700e-04  1.29938126e-05  1.46484381e-04
  2.68212482e-02  2.33281832e-02  1.88157428e-02  1.68969650e-02
  1.88215785e-02  1.63329542e-02  6.82399468e-03  8.09924304e-03]
training phase
Train Epoch: 11 [20000/50000] Loss: 28.249828 Acc: 0.8450 lr: 1.00e+00
Train Epoch: 11 [40000/50000] Loss: 33.167988 Acc: 0.7700 lr: 1.00e+00
Elapsed 1382.21s, 115.18 s/epoch, 0.46 s/batch, ets 4377.00s
weight distribution
[-0.0198864  -0.03759883 -0.01463369 -0.07047664 -0.0201391  -0.05822652
 -0.06304859  0.00613547  0.48101479  0.5294624   0.52105737  0.57405883
  0.53511328  0.54489368  0.68431395  0.61033332]
delta distribution
[ 4.44878498e-03 -1.48137414e-03 -1.01216638e-03 -1.13381284e-04
 -1.61276927e-04 -4.62532043e-05  5.28991222e-05 -2.44140625e-04
  3.58773619e-02  1.86573789e-02  2.10461151e-02  2.00542621e-02
  1.49629191e-02  1.27753159e-02  8.08560383e-03  6.87366351e-03]
training phase
Train Epoch: 12 [20000/50000] Loss: 39.173225 Acc: 0.7450 lr: 1.00e+00
Train Epoch: 12 [40000/50000] Loss: 31.030209 Acc: 0.8000 lr: 1.00e+00
Elapsed 1489.62s, 114.59 s/epoch, 0.46 s/batch, ets 4239.70s
weight distribution
[-0.02031238 -0.03946117 -0.01414728 -0.06819998 -0.02033303 -0.05938359
 -0.06588931  0.00491629  0.48442733  0.53038216  0.52126437  0.58124256
  0.53739846  0.54579449  0.68709093  0.60796815]
delta distribution
[ 4.21368657e-03  2.36511230e-04 -7.24792480e-05 -3.65787098e-04
 -1.81780924e-04  3.97443771e-04  3.36989760e-05 -2.01416013e-04
  3.61723974e-02  1.89641938e-02  1.90574639e-02  1.76577941e-02
  1.79464221e-02  1.55950710e-02  7.41591584e-03  1.18953157e-02]
training phase
Train Epoch: 13 [20000/50000] Loss: 36.424263 Acc: 0.7500 lr: 1.00e+00
Train Epoch: 13 [40000/50000] Loss: 35.319557 Acc: 0.7750 lr: 1.00e+00
Elapsed 1594.63s, 113.90 s/epoch, 0.46 s/batch, ets 4100.47s
weight distribution
[-0.01294979 -0.03889    -0.01757328 -0.07767161 -0.02193538 -0.06135411
 -0.06727281  0.00619168  0.47752666  0.5318498   0.52072847  0.58817279
  0.53902739  0.54468507  0.6849646   0.6054374 ]
delta distribution
[-9.22309060e-04  1.19866268e-03  3.62396240e-04  4.75141744e-04
 -2.08589772e-04 -2.29809026e-04 -2.46390700e-05 -9.76562515e-05
  2.92407125e-02  1.96002070e-02  2.05639880e-02  1.75977033e-02
  2.00201906e-02  1.67964492e-02  8.69857334e-03  1.16208410e-02]
training phase
Train Epoch: 14 [20000/50000] Loss: 30.890949 Acc: 0.8100 lr: 1.00e+00
Train Epoch: 14 [40000/50000] Loss: 33.523670 Acc: 0.7750 lr: 1.00e+00
Elapsed 1703.18s, 113.55 s/epoch, 0.45 s/batch, ets 3974.08s
weight distribution
[-0.012482   -0.04235699 -0.02043949 -0.07785451 -0.0252848  -0.06368598
 -0.06882279  0.00608397  0.47765857  0.53467315  0.52219421  0.58762443
  0.54191625  0.54467487  0.68637472  0.60410613]
delta distribution
[ 1.21166091e-03 -4.91672079e-04  1.52799825e-04 -2.10762024e-04
  5.34905354e-04  1.22017329e-04 -5.02169132e-06  2.07519537e-04
  3.01816501e-02  1.85253695e-02  1.59361064e-02  1.92846004e-02
  2.01201513e-02  1.92285217e-02  9.66954604e-03  1.06604882e-02]
training phase
Train Epoch: 15 [20000/50000] Loss: 25.910851 Acc: 0.8250 lr: 1.00e+00
Train Epoch: 15 [40000/50000] Loss: 34.273453 Acc: 0.7500 lr: 1.00e+00
Elapsed 1815.08s, 113.44 s/epoch, 0.45 s/batch, ets 3857.05s
weight distribution
[-0.0190479  -0.04514714 -0.02214642 -0.08164898 -0.02785807 -0.0637839
 -0.06945579  0.00753215  0.48564914  0.53869128  0.52073783  0.59233493
  0.54292375  0.54443491  0.68505156  0.60288125]
delta distribution
[-5.78703708e-04 -1.33387244e-03 -5.49528340e-04 -1.01407371e-04
 -2.33809158e-04 -2.56141037e-04 -3.69474292e-05 -3.05175781e-05
  3.02191339e-02  2.03130897e-02  2.26150118e-02  1.98380090e-02
  2.03341935e-02  1.11775203e-02  8.10981635e-03  1.08925542e-02]
training phase
Train Epoch: 16 [20000/50000] Loss: 27.929424 Acc: 0.8400 lr: 1.00e+00
Train Epoch: 16 [40000/50000] Loss: 38.294006 Acc: 0.7450 lr: 1.00e+00
Elapsed 1925.65s, 113.27 s/epoch, 0.45 s/batch, ets 3738.02s
weight distribution
[-0.01449843 -0.04436189 -0.02001627 -0.08593867 -0.02595055 -0.06646983
 -0.07227111  0.0087313   0.48451409  0.54270017  0.51890337  0.59559035
  0.54338229  0.54499972  0.68670219  0.59957033]
delta distribution
[-4.28602425e-03  1.77129114e-03 -1.53647532e-04 -2.28987803e-04
 -2.43716771e-04 -1.84641940e-05 -1.24573708e-05  2.86865252e-04
  3.70289274e-02  2.57422626e-02  2.12124139e-02  1.96595415e-02
  2.14217193e-02  1.16973128e-02  8.61638598e-03  1.03130089e-02]
training phase
Train Epoch: 17 [20000/50000] Loss: 23.297424 Acc: 0.8600 lr: 1.00e+00
Train Epoch: 17 [40000/50000] Loss: 33.074432 Acc: 0.7900 lr: 1.00e+00
Elapsed 2035.57s, 113.09 s/epoch, 0.45 s/batch, ets 3618.79s
weight distribution
[-0.01666188 -0.0392767  -0.02340227 -0.0866465  -0.02808563 -0.06439805
 -0.07119156  0.01019843  0.49203837  0.54214203  0.51893079  0.59379935
  0.54442292  0.54561549  0.68467438  0.59632254]
delta distribution
[ 3.12861684e-03  9.33753152e-04  9.68509266e-05  1.10838148e-04
 -8.21749345e-05 -6.38643920e-04 -6.72787428e-06 -2.50244135e-04
  2.74475720e-02  1.81477293e-02  2.28469558e-02  1.91588160e-02
  2.00597718e-02  1.62792839e-02  7.23391911e-03  1.10979341e-02]
training phase
Train Epoch: 18 [20000/50000] Loss: 26.504528 Acc: 0.8450 lr: 1.00e+00
Train Epoch: 18 [40000/50000] Loss: 28.567001 Acc: 0.8150 lr: 1.00e+00
Elapsed 2147.78s, 113.04 s/epoch, 0.45 s/batch, ets 3504.28s
weight distribution
[-0.01732359 -0.04322136 -0.02314398 -0.0861689  -0.03054936 -0.07034852
 -0.07292116  0.0088591   0.49694523  0.54206324  0.51904583  0.59322482
  0.54660302  0.54807687  0.68493724  0.59566009]
delta distribution
[-2.54991325e-03 -9.21461324e-04  5.23673196e-04 -7.30090687e-05
 -1.97410583e-04 -2.31001122e-05  1.55568123e-05  1.40380856e-04
  2.78342608e-02  1.88858472e-02  2.44250335e-02  1.84239987e-02
  1.61521044e-02  1.37509517e-02  8.76915827e-03  1.08566079e-02]
training phase
Train Epoch: 19 [20000/50000] Loss: 28.383488 Acc: 0.8350 lr: 1.00e+00
Train Epoch: 19 [40000/50000] Loss: 31.767477 Acc: 0.7950 lr: 1.00e+00
Elapsed 2262.22s, 113.11 s/epoch, 0.45 s/batch, ets 3393.33s
weight distribution
[-0.0184238  -0.04164236 -0.02388275 -0.08922511 -0.03067372 -0.06981254
 -0.07490106  0.00825371  0.48596594  0.54058647  0.51720983  0.59750396
  0.54496694  0.54987454  0.68639636  0.59367847]
delta distribution
[ 2.80309608e-03 -9.45197244e-05  1.16093957e-03  4.17179544e-04
  3.67111643e-04  6.21477739e-05  4.54634428e-05 -7.32421904e-05
  2.86114458e-02  1.92227960e-02  2.15767659e-02  2.02342086e-02
  1.52120441e-02  1.38835991e-02  7.49415020e-03  8.05300102e-03]
training phase
Train Epoch: 20 [20000/50000] Loss: 33.383888 Acc: 0.7850 lr: 1.00e+00
Train Epoch: 20 [40000/50000] Loss: 29.504009 Acc: 0.8150 lr: 1.00e+00
Elapsed 2377.35s, 113.21 s/epoch, 0.45 s/batch, ets 3283.01s
weight distribution
[-0.01442346 -0.03976661 -0.02187341 -0.09081847 -0.03070213 -0.07072269
 -0.07497212  0.00834101  0.4849267   0.54283828  0.51811177  0.5978052
  0.54534864  0.5512858   0.68573332  0.59514195]
delta distribution
[-1.37442129e-03  3.78078897e-04 -1.88615595e-04 -1.38812593e-05
 -1.43263082e-04 -9.95794908e-05 -7.08922744e-05 -2.13623047e-04
  2.74884999e-02  2.28043199e-02  2.08217036e-02  2.01124530e-02
  2.01560166e-02  1.92918777e-02  9.42094531e-03  1.12353535e-02]
training phase
Train Epoch: 21 [20000/50000] Loss: 31.957108 Acc: 0.7950 lr: 1.00e+00
Train Epoch: 21 [40000/50000] Loss: 30.744900 Acc: 0.8100 lr: 1.00e+00
Elapsed 2487.89s, 113.09 s/epoch, 0.45 s/batch, ets 3166.40s
weight distribution
[-0.00770431 -0.04275901 -0.02060136 -0.09064157 -0.0310231  -0.07205725
 -0.07484496  0.00797835  0.48967811  0.54612803  0.51757532  0.59860176
  0.5443517   0.55307543  0.68541914  0.59396034]
delta distribution
[-3.43605323e-04 -6.82830811e-04  1.01619295e-03 -4.13258858e-05
 -1.34785965e-04  4.76042442e-05 -1.58995390e-05 -1.46484381e-04
  2.65802909e-02  2.27768850e-02  2.71090716e-02  1.80616081e-02
  2.15658043e-02  1.86648183e-02  9.17177554e-03  1.09091122e-02]
training phase
Train Epoch: 22 [20000/50000] Loss: 24.159863 Acc: 0.8650 lr: 1.00e+00
Train Epoch: 22 [40000/50000] Loss: 31.453487 Acc: 0.7800 lr: 1.00e+00
Elapsed 2601.78s, 113.12 s/epoch, 0.45 s/batch, ets 3054.26s
weight distribution
[-0.01991407 -0.04330615 -0.02227413 -0.08917146 -0.03059611 -0.07458822
 -0.07583143  0.01001447  0.47622991  0.54481792  0.51731628  0.59793836
  0.54300869  0.55506784  0.6859206   0.59051502]
delta distribution
[-3.12861684e-03  1.93871395e-03  2.18709305e-04  5.05235454e-04
  8.36054460e-05  1.95609202e-04 -1.48639083e-05 -3.11279291e-04
  3.61575298e-02  2.46625114e-02  2.35655867e-02  1.71431489e-02
  1.55950850e-02  1.27215600e-02  6.36957260e-03  1.14683481e-02]
training phase
Train Epoch: 23 [20000/50000] Loss: 33.776180 Acc: 0.7750 lr: 1.00e+00
Train Epoch: 23 [40000/50000] Loss: 25.402626 Acc: 0.8450 lr: 1.00e+00
Elapsed 2717.60s, 113.23 s/epoch, 0.45 s/batch, ets 2944.06s
weight distribution
[-0.0082851  -0.03993449 -0.01986715 -0.08446173 -0.0307769  -0.07043299
 -0.07590999  0.00915023  0.48623595  0.54487813  0.51661819  0.59070146
  0.54133093  0.55444419  0.68523949  0.58995795]
delta distribution
[-2.07971642e-03  9.40958678e-04  2.11503779e-04  3.25838715e-04
  1.26997635e-04  3.44647306e-05  5.20497561e-05  4.88281257e-05
  3.33404168e-02  2.10965946e-02  2.08773892e-02  2.01052073e-02
  2.04303414e-02  1.56420693e-02  8.49245116e-03  1.08398227e-02]
training phase
Train Epoch: 24 [20000/50000] Loss: 29.386105 Acc: 0.8300 lr: 1.00e+00
Train Epoch: 24 [40000/50000] Loss: 25.253679 Acc: 0.8350 lr: 1.00e+00
Elapsed 2832.20s, 113.29 s/epoch, 0.45 s/batch, ets 2832.20s
weight distribution
[-0.01903196 -0.04504738 -0.02386531 -0.0906475  -0.03242787 -0.07420136
 -0.07672732  0.01087192  0.49726608  0.54455131  0.51620793  0.5930512
  0.54132575  0.55653775  0.68530917  0.58742881]
delta distribution
[-9.04224522e-04  6.48074667e-04 -1.45382350e-04  6.62273815e-05
  6.34193420e-05  9.51819966e-05 -4.64916229e-06 -3.84521496e-04
  3.08774207e-02  2.00514290e-02  2.32281834e-02  1.43682500e-02
  1.71750672e-02  1.85664464e-02  8.33107438e-03  1.01982122e-02]
training phase
Train Epoch: 25 [20000/50000] Loss: 28.339323 Acc: 0.8300 lr: 1.00e+00
Train Epoch: 25 [40000/50000] Loss: 28.625769 Acc: 0.8200 lr: 1.00e+00
Elapsed 2944.47s, 113.25 s/epoch, 0.45 s/batch, ets 2717.97s
weight distribution
[-0.02351228 -0.04204198 -0.02119201 -0.08518658 -0.0317336  -0.07354037
 -0.07736824  0.01018542  0.48922309  0.54681659  0.51728559  0.58769912
  0.54050791  0.55696917  0.68528706  0.58795381]
delta distribution
[ 1.04890042e-03 -1.17916532e-03 -3.17891431e-06  8.47710544e-06
 -7.49323110e-04  2.63982365e-04 -2.68295407e-05  4.27246086e-05
  3.08362003e-02  2.20644772e-02  1.98851340e-02  1.88679416e-02
  2.30305679e-02  1.75251551e-02  8.19746312e-03  9.22356918e-03]
training phase
Train Epoch: 26 [20000/50000] Loss: 27.244511 Acc: 0.8300 lr: 1.00e+00
Train Epoch: 26 [40000/50000] Loss: 31.698147 Acc: 0.7800 lr: 1.00e+00
Elapsed 3058.47s, 113.28 s/epoch, 0.45 s/batch, ets 2605.36s
weight distribution
[-0.01120532 -0.04194874 -0.0230857  -0.08708024 -0.03200863 -0.07444579
 -0.07745272  0.01047493  0.49286753  0.547095    0.51834428  0.58998346
  0.5404799   0.55999994  0.68554538  0.58641666]
delta distribution
[-1.44675933e-03  7.81589071e-04 -2.64909526e-04  1.59687464e-04
  3.87297732e-05 -1.79237788e-04  1.41113997e-05  4.08935564e-04
  2.34444644e-02  1.86919775e-02  2.35465579e-02  1.78905521e-02
  1.53185790e-02  1.61380600e-02  8.14074930e-03  1.01597803e-02]
training phase
Train Epoch: 27 [20000/50000] Loss: 27.517754 Acc: 0.8450 lr: 1.00e+00
Train Epoch: 27 [40000/50000] Loss: 28.197573 Acc: 0.8350 lr: 1.00e+00
Elapsed 3171.95s, 113.28 s/epoch, 0.45 s/batch, ets 2492.25s
weight distribution
[-0.01435382 -0.04309142 -0.02493818 -0.08705942 -0.03058042 -0.07307523
 -0.07953407  0.00975444  0.49224952  0.54858732  0.51882052  0.58918124
  0.53958243  0.56047523  0.68415254  0.58751255]
delta distribution
[ 4.24985541e-03  9.74867071e-05 -2.45836054e-05  2.26444667e-04
 -1.51899134e-04  1.22997502e-04  2.70530581e-05  1.28173837e-04
  3.18816490e-02  2.39073876e-02  2.16079634e-02  2.02493444e-02
  1.56717747e-02  1.50739085e-02  6.25898503e-03  8.40014033e-03]
training phase
Train Epoch: 28 [20000/50000] Loss: 27.395664 Acc: 0.8150 lr: 1.00e+00
Train Epoch: 28 [40000/50000] Loss: 25.282255 Acc: 0.8400 lr: 1.00e+00
Elapsed 3286.72s, 113.34 s/epoch, 0.45 s/batch, ets 2380.04s
weight distribution
[-0.01259539 -0.0357937  -0.02440032 -0.08746471 -0.03057645 -0.074963
 -0.07913023  0.01136729  0.49239072  0.55151933  0.51855671  0.58404702
  0.53963459  0.56319213  0.68506241  0.58599317]
delta distribution
[-2.89351866e-03 -1.54707173e-03  5.27699776e-05  2.05569795e-05
  2.69148095e-05  7.24527563e-05 -1.46999955e-05 -2.07519537e-04
  2.72045489e-02  2.39859372e-02  1.73562709e-02  1.53094307e-02
  1.68971121e-02  1.24551523e-02  6.18899753e-03  9.60633252e-03]
training phase
Train Epoch: 29 [20000/50000] Loss: 20.220356 Acc: 0.9000 lr: 1.00e+00
Train Epoch: 29 [40000/50000] Loss: 30.982292 Acc: 0.7900 lr: 1.00e+00
Elapsed 3401.97s, 113.40 s/epoch, 0.45 s/batch, ets 2267.98s
weight distribution
[-0.01521725 -0.04172426 -0.02443179 -0.08638624 -0.03279293 -0.07685784
 -0.07929742  0.01116849  0.49450687  0.55296993  0.51756781  0.58349496
  0.54132456  0.56666625  0.68613857  0.58585203]
delta distribution
[ 1.30208337e-03  5.68389893e-04 -1.59793432e-04 -1.83741256e-04
  3.83853912e-04 -9.24004489e-05  1.84997916e-05 -6.40869141e-04
  3.25394273e-02  2.58143246e-02  2.40203235e-02  1.52559048e-02
  2.09283736e-02  1.12517430e-02  7.36025162e-03  1.18797449e-02]
training phase
Train Epoch: 30 [20000/50000] Loss: 24.730648 Acc: 0.8400 lr: 1.00e+00
Train Epoch: 30 [40000/50000] Loss: 24.685997 Acc: 0.8300 lr: 1.00e+00
Elapsed 3516.66s, 113.44 s/epoch, 0.45 s/batch, ets 2155.37s
weight distribution
[-0.01733259 -0.04354198 -0.02561734 -0.08799233 -0.03076631 -0.0740851
 -0.08118618  0.01181776  0.48927912  0.54878455  0.51558858  0.58286822
  0.54041332  0.56458879  0.68567055  0.58532006]
delta distribution
[ 1.37442129e-03  1.49620906e-03 -1.01513331e-04  3.30183248e-04
  1.63926015e-04  1.36163499e-05 -4.33772802e-05 -1.28173837e-04
  3.44274379e-02  2.30405927e-02  2.19721571e-02  1.33878682e-02
  1.75605938e-02  1.85962226e-02  8.62222817e-03  1.04265558e-02]
training phase
Train Epoch: 31 [20000/50000] Loss: 23.877371 Acc: 0.8500 lr: 1.00e+00
Train Epoch: 31 [40000/50000] Loss: 26.198715 Acc: 0.8450 lr: 1.00e+00
Elapsed 3629.97s, 113.44 s/epoch, 0.45 s/batch, ets 2041.86s
weight distribution
[-0.01742106 -0.04356037 -0.02128782 -0.0843714  -0.03137814 -0.07546142
 -0.08108123  0.01142273  0.49971271  0.54922068  0.51861739  0.58149511
  0.54123741  0.56577647  0.68651944  0.58461064]
delta distribution
[-7.77633104e-04  1.92218355e-03  2.60035187e-04  1.41991506e-04
 -1.07447304e-04  9.62681224e-05 -4.50238585e-05  9.70459019e-04
  2.87772603e-02  1.99217871e-02  1.76443011e-02  1.43826725e-02
  1.64841209e-02  1.74332522e-02  7.64112500e-03  1.57779958e-02]
training phase
Train Epoch: 32 [20000/50000] Loss: 23.887150 Acc: 0.8550 lr: 1.00e+00
Train Epoch: 32 [40000/50000] Loss: 30.231470 Acc: 0.7950 lr: 1.00e+00
Elapsed 3745.77s, 113.51 s/epoch, 0.45 s/batch, ets 1929.64s
weight distribution
[-0.01390673 -0.04296147 -0.02347775 -0.08626216 -0.03161705 -0.07366367
 -0.08316103  0.0127695   0.48753834  0.55016899  0.5168938   0.58215106
  0.54172689  0.56397045  0.68719077  0.58275986]
delta distribution
[ 3.81582766e-03  9.51131165e-04  2.36511230e-04  2.94473430e-04
 -4.09020322e-05 -2.95109232e-04 -5.04478812e-05 -6.10351572e-06
  2.90052630e-02  1.94605570e-02  2.49485578e-02  2.03714054e-02
  1.55254537e-02  1.73226856e-02  8.03550333e-03  1.12033822e-02]
training phase
Train Epoch: 33 [20000/50000] Loss: 28.811886 Acc: 0.7850 lr: 1.00e+00
Train Epoch: 33 [40000/50000] Loss: 29.489481 Acc: 0.8150 lr: 1.00e+00
Elapsed 3860.03s, 113.53 s/epoch, 0.45 s/batch, ets 1816.48s
weight distribution
[-0.01016135 -0.04038345 -0.02222543 -0.08841354 -0.03100626 -0.07344746
 -0.08436552  0.01284256  0.49042073  0.54913926  0.51611805  0.58337766
  0.54267436  0.56654119  0.68657631  0.58423191]
delta distribution
[-5.75086800e-03 -3.22553853e-04  2.54313141e-04  3.99695506e-04
 -1.80138488e-06  9.57383090e-05  4.30047512e-05 -2.13623047e-04
  2.95960587e-02  2.61509176e-02  2.20692698e-02  1.75343584e-02
  1.69983450e-02  1.69333685e-02  5.99839445e-03  1.23359496e-02]
training phase
Train Epoch: 34 [20000/50000] Loss: 28.484051 Acc: 0.8000 lr: 1.00e+00
Train Epoch: 34 [40000/50000] Loss: 27.475355 Acc: 0.8000 lr: 1.00e+00
Elapsed 3974.84s, 113.57 s/epoch, 0.45 s/batch, ets 1703.50s
weight distribution
[-0.01998375 -0.04155225 -0.02196253 -0.08621681 -0.03202954 -0.07372462
 -0.0851434   0.0125413   0.49087155  0.55344981  0.51685953  0.5778839
  0.54323781  0.56445396  0.68600029  0.58134502]
delta distribution
[-3.05627892e-03 -9.65966145e-04  2.63214111e-04 -8.09563571e-05
 -1.17619835e-04 -2.44087641e-04 -8.38190317e-06 -2.13623047e-04
  3.32310870e-02  2.57500913e-02  2.29546744e-02  1.44767743e-02
  1.60764419e-02  1.64078567e-02  6.15331810e-03  1.08202128e-02]
training phase
Train Epoch: 35 [20000/50000] Loss: 27.210152 Acc: 0.8200 lr: 1.00e+00
Train Epoch: 35 [40000/50000] Loss: 21.536238 Acc: 0.8600 lr: 1.00e+00
Elapsed 4089.59s, 113.60 s/epoch, 0.45 s/batch, ets 1590.40s
weight distribution
[-0.00898007 -0.04262349 -0.02292179 -0.08673211 -0.03230783 -0.07437716
 -0.08526525  0.01263162  0.49203205  0.5530436   0.51749855  0.57581162
  0.5445987   0.56604129  0.68881011  0.58203876]
delta distribution
[-7.95717631e-03  2.54313141e-04  5.50587953e-04  1.37117182e-04
 -3.45442022e-05  2.67293726e-05  2.97650695e-05 -4.27246086e-05
  3.25998552e-02  2.30854265e-02  2.20938604e-02  2.03875322e-02
  1.53059382e-02  1.77297127e-02  7.66681274e-03  1.13722924e-02]
training phase
Train Epoch: 36 [20000/50000] Loss: 26.830978 Acc: 0.8500 lr: 1.00e+00
Train Epoch: 36 [40000/50000] Loss: 22.609909 Acc: 0.8800 lr: 1.00e+00
Elapsed 4204.62s, 113.64 s/epoch, 0.45 s/batch, ets 1477.30s
weight distribution
[-0.01602646 -0.04455037 -0.02353024 -0.08672865 -0.03362564 -0.07514545
 -0.08624376  0.01435877  0.49501657  0.55025578  0.5167951   0.57627159
  0.54452127  0.56497085  0.68801469  0.57998645]
delta distribution
[-1.66377320e-03  3.43322754e-05 -1.30759348e-04  3.36753001e-04
  8.59366555e-05  7.72211279e-05  1.27553940e-05  0.00000000e+00
  2.61656381e-02  1.79014541e-02  2.28873342e-02  1.87896602e-02
  1.78539958e-02  1.57633834e-02  6.10624859e-03  6.53672544e-03]
training phase
Train Epoch: 37 [20000/50000] Loss: 23.940117 Acc: 0.8650 lr: 1.00e+00
Train Epoch: 37 [40000/50000] Loss: 28.321079 Acc: 0.8250 lr: 1.00e+00
Elapsed 4318.75s, 113.65 s/epoch, 0.45 s/batch, ets 1363.81s
weight distribution
[-0.01009879 -0.04330894 -0.02111192 -0.08659003 -0.03552183 -0.07226948
 -0.08848954  0.01491128  0.49462235  0.55334026  0.5167855   0.57754475
  0.54585207  0.5650143   0.68923181  0.57966965]
delta distribution
[ 7.23379664e-04 -3.43322754e-05 -1.06811523e-04  7.68237660e-05
 -4.42292949e-04 -9.71158370e-05  2.13831663e-05  1.40380856e-04
  2.67634429e-02  2.32725181e-02  2.40866970e-02  1.98207516e-02
  1.56813767e-02  1.82092823e-02  5.82154421e-03  1.19601591e-02]
training phase
Train Epoch: 38 [20000/50000] Loss: 29.016891 Acc: 0.8000 lr: 1.00e+00
Train Epoch: 38 [40000/50000] Loss: 29.121161 Acc: 0.7800 lr: 1.00e+00
Elapsed 4431.87s, 113.64 s/epoch, 0.45 s/batch, ets 1250.02s
weight distribution
[-0.0161803  -0.04067234 -0.02197616 -0.08829787 -0.03396562 -0.07016946
 -0.08782016  0.01483194  0.49362031  0.55095792  0.51789576  0.57628798
  0.54597068  0.56199992  0.68826222  0.58121175]
delta distribution
[-7.05295126e-04 -8.66360147e-04 -2.82499532e-04  1.16560193e-06
 -2.56591389e-04  6.99361181e-06 -1.03041530e-05 -1.46484381e-04
  3.20856087e-02  1.92041844e-02  1.76477078e-02  2.04785820e-02
  1.67068616e-02  1.14016552e-02  7.41416356e-03  1.02603352e-02]
training phase
Train Epoch: 39 [20000/50000] Loss: 24.543722 Acc: 0.8400 lr: 1.00e+00
Train Epoch: 39 [40000/50000] Loss: 23.293245 Acc: 0.8450 lr: 1.00e+00
weight distribution
[-0.01188759 -0.04298631 -0.02158126 -0.08375488 -0.03117527 -0.07049997
 -0.08890425  0.01443973  0.49271554  0.55405635  0.51775312  0.57198757
  0.54544896  0.56237334  0.68734366  0.58120161]
delta distribution
[-1.53718167e-03  1.34362112e-04 -1.97092686e-05  4.00649180e-04
  8.36584295e-05  9.97914249e-05  3.68729234e-05  2.19726571e-04
  2.98276041e-02  2.39011124e-02  2.19790116e-02  2.20254753e-02
  1.50340935e-02  1.39520802e-02  6.14207843e-03  1.07670724e-02]
Elapsed 4547.33s, 113.68 s/epoch, 0.45 s/batch, ets 1136.83s
training phase
Train Epoch: 40 [20000/50000] Loss: 25.274162 Acc: 0.8550 lr: 1.00e+00
Train Epoch: 40 [40000/50000] Loss: 23.432667 Acc: 0.8600 lr: 1.00e+00
Elapsed 4662.30s, 113.71 s/epoch, 0.45 s/batch, ets 1023.43s
weight distribution
[-0.0162325  -0.03996284 -0.02251789 -0.08607528 -0.03269273 -0.07227413
 -0.08768349  0.01504905  0.49899176  0.55303288  0.51816839  0.57400137
  0.54584581  0.56208467  0.68617231  0.58063084]
delta distribution
[-5.51576959e-03 -3.89522989e-04 -1.43686935e-04  5.20282338e-05
  1.12162699e-04  3.02314758e-04 -2.62334943e-05  5.49316428e-05
  2.70271450e-02  1.70155466e-02  1.87289547e-02  1.62537806e-02
  1.77499615e-02  1.39416233e-02  7.09747104e-03  1.20561663e-02]
training phase
Train Epoch: 41 [20000/50000] Loss: 29.619654 Acc: 0.8000 lr: 1.00e+00
Train Epoch: 41 [40000/50000] Loss: 32.517117 Acc: 0.7850 lr: 1.00e+00
Elapsed 4777.54s, 113.75 s/epoch, 0.46 s/batch, ets 910.01s
weight distribution
[-0.01537361 -0.04197473 -0.02281704 -0.08804308 -0.03453612 -0.06797264
 -0.08993345  0.01479393  0.49406293  0.55305123  0.5183599   0.57288355
  0.5489589   0.56167221  0.68800086  0.58042723]
delta distribution
[-1.84461812e-03  1.52333581e-03 -1.73144872e-04 -2.67346710e-04
  1.72350148e-04 -2.75956263e-04 -6.05732203e-06 -1.22070314e-05
  2.92936787e-02  2.47259587e-02  2.27634497e-02  2.04305369e-02
  1.50401639e-02  1.93622950e-02  6.53243531e-03  9.92112141e-03]
training phase
Train Epoch: 42 [20000/50000] Loss: 30.503788 Acc: 0.7900 lr: 1.00e+00
Train Epoch: 42 [40000/50000] Loss: 29.821569 Acc: 0.8300 lr: 1.00e+00
Elapsed 4892.33s, 113.78 s/epoch, 0.46 s/batch, ets 796.43s
weight distribution
[-0.02410968 -0.04467818 -0.02435971 -0.08853002 -0.03386311 -0.06945716
 -0.08925472  0.01524491  0.49300188  0.55640113  0.51815629  0.57206059
  0.54951888  0.56048977  0.68998748  0.57911491]
delta distribution
[ 5.94979757e-03  9.37991659e-04 -1.27156572e-05 -2.93095916e-04
 -1.05222069e-04 -2.59055028e-04  2.11372972e-05 -2.56347674e-04
  3.21405344e-02  2.31291223e-02  2.15938687e-02  1.83036942e-02
  1.56413857e-02  1.79646034e-02  6.59638923e-03  7.45884050e-03]
training phase
Train Epoch: 43 [20000/50000] Loss: 25.506659 Acc: 0.8300 lr: 1.00e+00
Train Epoch: 43 [40000/50000] Loss: 25.594318 Acc: 0.8300 lr: 1.00e+00
Elapsed 5006.48s, 113.78 s/epoch, 0.46 s/batch, ets 682.70s
weight distribution
[-0.00927841 -0.04141589 -0.02238898 -0.08367135 -0.03234423 -0.06877927
 -0.09092155  0.01571402  0.4921194   0.55508494  0.51930553  0.57500213
  0.54903615  0.5605824   0.6875726   0.57928592]
delta distribution
[-1.32016779e-03  4.73022461e-04  4.63485718e-04  2.74340302e-04
 -1.24295548e-04  2.23689611e-04 -1.95726752e-05  1.83105476e-05
  3.15687098e-02  1.88096724e-02  2.33556721e-02  1.59525350e-02
  1.61755718e-02  1.41825788e-02  8.16390011e-03  1.08575011e-02]
training phase
Train Epoch: 44 [20000/50000] Loss: 30.064161 Acc: 0.8250 lr: 1.00e+00
Train Epoch: 44 [40000/50000] Loss: 23.447063 Acc: 0.8650 lr: 1.00e+00
Elapsed 5121.63s, 113.81 s/epoch, 0.46 s/batch, ets 569.07s
weight distribution
[-0.01712218 -0.04218062 -0.0202427  -0.08736585 -0.03517227 -0.06795885
 -0.09060998  0.0158779   0.48939249  0.55081099  0.51651233  0.57153362
  0.55095088  0.56177467  0.68727392  0.57789892]
delta distribution
[ 2.92968750e-03 -3.53919138e-04  7.81165261e-04 -3.11851501e-04
  1.14652845e-04  1.39395401e-04  5.30853868e-05  2.38037115e-04
  2.92437822e-02  2.44533233e-02  2.35424023e-02  1.92591734e-02
  1.09739732e-02  1.34101100e-02  8.47441331e-03  1.16679613e-02]
training phase
Train Epoch: 45 [20000/50000] Loss: 26.014496 Acc: 0.8550 lr: 1.00e+00
Train Epoch: 45 [40000/50000] Loss: 19.928493 Acc: 0.8950 lr: 1.00e+00
Elapsed 5236.21s, 113.83 s/epoch, 0.46 s/batch, ets 455.32s
weight distribution
[-0.02067607 -0.04174358 -0.02409036 -0.08496027 -0.03415149 -0.06817356
 -0.09133468  0.01735134  0.49431863  0.55207962  0.51857615  0.57101619
  0.54978973  0.56144667  0.68820798  0.57808352]
delta distribution
[ 6.32957206e-04 -4.20506811e-03 -1.63184275e-04 -4.70373372e-04
 -1.22282247e-04 -2.14656189e-04  1.87009573e-05  1.64794925e-04
  2.87414957e-02  2.35860273e-02  1.61609128e-02  1.97023638e-02
  1.64502356e-02  1.34570012e-02  6.18782034e-03  8.57925881e-03]
training phase
Train Epoch: 46 [20000/50000] Loss: 30.056650 Acc: 0.8000 lr: 1.00e+00
Train Epoch: 46 [40000/50000] Loss: 24.758455 Acc: 0.8400 lr: 1.00e+00
Elapsed 5349.26s, 113.81 s/epoch, 0.46 s/batch, ets 341.44s
weight distribution
[-0.01823747 -0.04207832 -0.02283402 -0.08771942 -0.0356552  -0.07012156
 -0.09202109  0.01666961  0.49906826  0.55203629  0.5176816   0.57186198
  0.55126983  0.56187385  0.68667358  0.57852411]
delta distribution
[ 4.62962966e-03  9.22309060e-04 -1.33938258e-04 -3.62926075e-04
  1.46759885e-05  7.71151681e-05  2.18302011e-05 -6.10351562e-05
  2.87098419e-02  2.47213971e-02  2.21224781e-02  1.81043427e-02
  1.73291136e-02  1.77249890e-02  6.35018619e-03  8.77842121e-03]
training phase
Train Epoch: 47 [20000/50000] Loss: 20.283840 Acc: 0.8850 lr: 1.00e+00
Train Epoch: 47 [40000/50000] Loss: 22.546146 Acc: 0.8800 lr: 1.00e+00
Elapsed 5464.66s, 113.85 s/epoch, 0.46 s/batch, ets 227.69s
weight distribution
[-0.01671764 -0.03728961 -0.02288353 -0.08784216 -0.03558876 -0.06942461
 -0.09147698  0.01628623  0.49374589  0.55673045  0.51842439  0.57370645
  0.5514763   0.56310713  0.68669242  0.57850027]
delta distribution
[ 4.06901026e-03 -2.16590037e-04  1.30123561e-04 -3.88781220e-04
 -2.63637980e-04  8.29166820e-05  4.61190939e-05 -2.44140629e-05
  2.89317686e-02  1.91914644e-02  2.59190258e-02  1.79546680e-02
  1.95775740e-02  1.42865675e-02  6.82903547e-03  1.17518054e-02]
training phase
Train Epoch: 48 [20000/50000] Loss: 26.658913 Acc: 0.8250 lr: 1.00e+00
Train Epoch: 48 [40000/50000] Loss: 25.103485 Acc: 0.8650 lr: 1.00e+00
Elapsed 5579.56s, 113.87 s/epoch, 0.46 s/batch, ets 113.87s
weight distribution
[-0.01865105 -0.04267703 -0.02550941 -0.08629489 -0.03517146 -0.06782565
 -0.09367185  0.01736443  0.49313301  0.55770653  0.51849109  0.57218677
  0.55299997  0.56209552  0.68765187  0.57642561]
delta distribution
[-4.46686940e-03 -1.66871818e-03  2.68088450e-04 -6.41928811e-04
 -6.70750960e-05 -1.93834305e-04  2.86772847e-05 -4.15039074e-04
  3.45097370e-02  2.35727578e-02  2.09005177e-02  1.99668445e-02
  2.17867102e-02  1.73099134e-02  7.01018935e-03  1.03271334e-02]
training phase
Train Epoch: 49 [20000/50000] Loss: 34.011852 Acc: 0.7800 lr: 1.00e+00
Train Epoch: 49 [40000/50000] Loss: 26.510044 Acc: 0.8050 lr: 1.00e+00
weight distribution
[-0.01963337 -0.04107429 -0.02210631 -0.08879597 -0.03616299 -0.07034657
 -0.09316093  0.01718398  0.4997541   0.5535261   0.51875502  0.57308626
  0.55566567  0.56369889  0.68692219  0.57666332]
delta distribution
[ 2.02546292e-03 -1.89632841e-03  4.64757293e-04 -1.80668299e-04
 -2.08748716e-05 -1.12639536e-04  1.90660357e-05  2.13623047e-04
  2.57917847e-02  2.36850791e-02  2.07296982e-02  1.82142928e-02
  1.80852711e-02  1.59327816e-02  8.60823598e-03  8.71058274e-03]
Elapsed 5694.70s, 113.89 s/epoch, 0.46 s/batch, ets 0.00s
testing phase
	Epoch 49 Test set: Average loss: 26.9423, Accuracy: 8229/10000 (82%)
Saving model to /home/chwolters/BachelorThesis/Training_pytorch/log/default/ADCprecision=5/batch_size=200/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5/best-49.pth
------------------------------ FloorPlan --------------------------------
Tile and PE size are optimized to maximize memory utilization ( = memory mapped by synapse / total memory on chip)
Desired Conventional Mapped Tile Storage Size: 1024x1024
Desired Conventional PE Storage Size: 512x512
Desired Novel Mapped Tile Storage Size: 9x512x512
User-defined SubArray Size: 128x128
----------------- # of tile used for each layer -----------------
layer1: 1
layer2: 1
layer3: 2
layer4: 2
layer5: 3
layer6: 3
layer7: 24
layer8: 1
----------------- Speed-up of each layer ------------------
layer1: 16
layer2: 4
layer3: 4
layer4: 2
layer5: 2
layer6: 1
layer7: 1
layer8: 8
----------------- Utilization of each layer ------------------
layer1: 0.158203
layer2: 0.75
layer3: 0.75
layer4: 0.75
layer5: 1
layer6: 1
layer7: 1
layer8: 0.234375
Memory Utilization of Whole Chip: 92.2772 %
---------------------------- FloorPlan Done ------------------------------
-------------------------------------- Hardware Performance --------------------------------------
